# Google Colab Troubleshooting Guide

**For RNN Next-Word Prediction Assignment**

---

## âœ… Error Fixed!

### Issue: `ValueError: You tried to call count_params on layer...but the layer isn't built`

**What happened:** The model wasn't built before trying to count parameters.

**Fix applied:** Added `model.build(input_shape=(None, SEQUENCE_LENGTH))` before counting parameters.

**Status:** âœ… FIXED - Notebook is now Colab-ready!

---

## ðŸš€ Quick Start for Google Colab

### Step 1: Upload Notebook

1. Go to https://colab.research.google.com/
2. Click **File â†’ Upload notebook**
3. Select `RNN_NextWord_Complete_Assignment.ipynb`

### Step 2: Enable GPU (IMPORTANT!)

```
Runtime â†’ Change runtime type â†’ Hardware accelerator â†’ GPU (T4)
```

**Why:** Training will be 10-20x faster with GPU!

### Step 3: Download GloVe Embeddings

**Add this as a NEW CELL at the top** (after imports):

```python
# Download GloVe 100D embeddings (run once)
!wget http://nlp.stanford.edu/data/glove.6B.zip
!unzip -q glove.6B.zip
!ls -lh glove.6B.100d.txt

print("âœ“ GloVe embeddings downloaded!")
```

**Time:** ~2 minutes to download

### Step 4: Run All Cells

```
Runtime â†’ Run all (or Ctrl+F9)
```

**Training time:** ~10-15 minutes with GPU

---

## ðŸ”§ Common Colab Errors & Fixes

### Error 1: GloVe File Not Found

**Symptoms:**
```
âš  GloVe file not found!
Using random initialization instead...
```

**Solution:**
Run this cell BEFORE starting training:
```python
!wget http://nlp.stanford.edu/data/glove.6B.zip
!unzip -q glove.6B.zip
```

**Why:** GloVe embeddings improve accuracy by 15-20%

---

### Error 2: Out of Memory

**Symptoms:**
```
ResourceExhaustedError: OOM when allocating tensor...
```

**Solution 1:** Reduce batch size (in Configuration cell):
```python
BATCH_SIZE = 64  # Instead of 128
```

**Solution 2:** Reduce vocabulary:
```python
MAX_VOCAB_SIZE = 5000  # Instead of 10000
```

**Solution 3:** Use GPU instead of CPU:
```
Runtime â†’ Change runtime type â†’ GPU
```

---

### Error 3: Session Timeout

**Symptoms:**
```
Your session has been disconnected...
```

**Cause:** Colab disconnects after ~90 minutes of inactivity

**Solution:**
1. **Save checkpoints:** Model automatically saves to `saved_models/best_model.keras`
2. **Download immediately:** Right-click saved_models folder â†’ Download
3. **Keep active:** Click cells occasionally to prevent timeout

**Prevention:**
```python
# Add this to keep session active
from google.colab import files
import time

# At end of training, auto-download model
files.download('saved_models/best_model.keras')
files.download('saved_models/tokenizer.pkl')
```

---

### Error 4: Graphviz Not Found

**Symptoms:**
```
âš  Could not generate model diagram (graphviz may not be installed)
```

**Solution (optional):**
```python
!apt-get install -y graphviz
!pip install pydot graphviz
```

**Note:** This is optional - doesn't affect training

---

### Error 5: Runtime Crashed

**Symptoms:**
```
Your session crashed after using all available RAM
```

**Solution:** Reduce data size:
```python
# In create_sequences function, limit sequences:
sequences = sequences[:100000]  # Use only first 100k sequences
```

Or increase RAM:
```
Runtime â†’ Change runtime type â†’ High-RAM (if available)
```

---

## ðŸ“Š Expected Training Output

### Phase 1: Data Loading (1-2 min)
```
Loading Shakespeare dataset...
âœ“ Loaded 1,115,394 characters
Cleaning text...
âœ“ Text cleaned: 1,003,854 characters remaining
```

### Phase 2: Tokenization (1 min)
```
Creating tokenizer with max 10,000 words...
âœ“ Vocabulary size: 31,894 unique words
  Using top 10,000 words
```

### Phase 3: Sequence Generation (2-3 min)
```
Creating sequences (length=50)...
  Total tokens: 200,258
  Generated 10,000 sequences...
  Generated 20,000 sequences...
...
âœ“ Created 200,208 training sequences
```

### Phase 4: GloVe Loading (2 min)
```
Loading GloVe embeddings from glove.6B.100d.txt...
  Loaded 10,000 word vectors...
  Loaded 20,000 word vectors...
...
âœ“ Loaded 400,000 word vectors
```

### Phase 5: Model Building (instant)
```
Building LSTM model...
âœ“ Model built successfully!

Model: "RNN_NextWord_Predictor"
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 embedding (Embedding)       (None, 50, 100)           1,000,000
 masking (Masking)           (None, 50, 100)           0
 lstm (LSTM)                 (None, 256)               365,568
 dense_relu (Dense)          (None, 128)               32,896
 dropout (Dropout)           (None, 128)               0
 output (Dense)              (None, 10000)             1,290,000
=================================================================
Total params: 2,688,464
Trainable params: 1,688,464 (frozen embeddings: 1,000,000)
```

### Phase 6: Training (10-15 min with GPU)
```
Epoch 1/50
1251/1251 [==============================] - 45s 35ms/step
loss: 6.2341 - accuracy: 0.1234 - val_loss: 5.8765 - val_accuracy: 0.1543

Epoch 2/50
1251/1251 [==============================] - 42s 33ms/step
loss: 5.6789 - accuracy: 0.1876 - val_loss: 5.3456 - val_accuracy: 0.2134
...

Training Complete!
Final validation accuracy: 0.4523 (45.23%)
Final top-5 accuracy: 0.7812 (78.12%)
```

---

## ðŸ’¾ Downloading Results from Colab

### Method 1: Manual Download

After training completes:
```
1. Click folder icon (left sidebar)
2. Navigate to saved_models/
3. Right-click each file â†’ Download:
   - final_model.h5
   - best_model.keras
   - tokenizer.pkl
   - config.json
   - history.pkl
```

### Method 2: Automatic Download

Add this cell at the END of the notebook:
```python
# Auto-download trained model files
from google.colab import files

print("Downloading model files...")
files.download('saved_models/final_model.h5')
files.download('saved_models/tokenizer.pkl')
files.download('saved_models/config.json')
print("âœ“ Downloads complete!")
```

### Method 3: Save to Google Drive

Add this cell BEFORE training:
```python
# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Update MODEL_DIR to save to Drive
MODEL_DIR = '/content/drive/MyDrive/RNN_Models/'
os.makedirs(MODEL_DIR, exist_ok=True)
```

**Result:** Models automatically save to your Google Drive!

---

## âš¡ Performance Tips

### 1. Use GPU Runtime
```
Runtime â†’ Change runtime type â†’ GPU (T4)
```
**Speed:** 10-20x faster than CPU

### 2. Reduce Training Time
```python
EPOCHS = 20  # Instead of 50
SEQUENCE_LENGTH = 30  # Instead of 50
```
**Time saved:** 50-60%

### 3. Monitor GPU Usage
```python
# Add this cell to check GPU
!nvidia-smi
```

### 4. Prevent Disconnection
```python
# Run this in background to keep session alive
import IPython
js_code = '''
function ClickConnect(){
  console.log("Keeping session alive...");
  document.querySelector("colab-connect-button").click()
}
setInterval(ClickConnect, 60000)
'''
IPython.display.display(IPython.display.Javascript(js_code))
```

---

## ðŸ“ˆ Interpreting Results

### Good Performance Indicators:

âœ… **Training accuracy: 40-60%**
- Random guessing would be 0.01% (1 in 10,000)
- 45% is excellent!

âœ… **Top-5 accuracy: 70-85%**
- Correct word in top 5 predictions
- Very good for autocomplete

âœ… **Validation loss decreasing**
- Should drop from ~6.0 to ~3.5-4.5
- Lower is better

âœ… **Low overfitting**
- Training loss â‰ˆ Validation loss
- Difference < 0.5 is ideal

### Warning Signs:

âš ï¸ **Accuracy < 20%**
- Model isn't learning
- Check: GloVe loaded? GPU enabled? Enough epochs?

âš ï¸ **Loss not decreasing**
- Learning rate too high/low
- Try: Reduce LEARNING_RATE to 0.0005

âš ï¸ **Huge overfitting** (train loss << val loss)
- Model memorizing training data
- Increase DROPOUT_RATE to 0.3

---

## ðŸŽ¯ Quick Checklist

Before submitting assignment:

- [ ] GPU runtime enabled
- [ ] GloVe embeddings downloaded
- [ ] All cells run without errors
- [ ] Training completed (reached end)
- [ ] Validation accuracy > 35%
- [ ] Model files downloaded:
  - [ ] final_model.h5
  - [ ] tokenizer.pkl
  - [ ] config.json
- [ ] Visualizations generated:
  - [ ] training_performance.png
- [ ] Text generation examples working
- [ ] Notebook exported as PDF/HTML

---

## ðŸ†˜ Still Having Issues?

### Check Python Version:
```python
import sys
print(f"Python version: {sys.version}")
# Should be Python 3.10+
```

### Check TensorFlow:
```python
import tensorflow as tf
print(f"TensorFlow: {tf.__version__}")
print(f"GPU: {tf.config.list_physical_devices('GPU')}")
# Should show: TensorFlow 2.13+ with GPU
```

### Reset Everything:
```
Runtime â†’ Restart runtime
Runtime â†’ Run all
```

### Last Resort:
1. Download notebook
2. Create NEW Colab notebook
3. Upload fresh copy
4. Enable GPU
5. Run all cells

---

## ðŸ“ž Common Questions

**Q: How long should training take?**
A: With GPU: 10-15 minutes. With CPU: 2-3 hours.

**Q: Can I stop and resume training?**
A: Yes! Model checkpoints save automatically. Load with:
```python
model = keras.models.load_model('saved_models/best_model.keras')
```

**Q: Do I need GloVe embeddings?**
A: No, but highly recommended. Without GloVe:
- Accuracy: 30-40% (vs 45-55% with GloVe)
- Training time: +30% longer

**Q: What if my accuracy is only 30%?**
A: Still acceptable! For 10,000 classes:
- 30% = good
- 40% = very good
- 50%+ = excellent

**Q: Can I use my own text data?**
A: Yes! Replace Shakespeare download with your text file.

---

## âœ… Success!

Once you see:
```
âœ“ Model saved to saved_models/final_model.h5
âœ“ Tokenizer saved to saved_models/tokenizer.pkl
âœ“ Config saved to saved_models/config.json
```

**You're done!** Download the files and your assignment is complete.

---

**Good luck!** ðŸš€

*Updated: November 2, 2025*
*Error fix: model.build() added to prevent count_params error*
